{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c10b768-f5b4-43bc-a785-99077422ce78",
   "metadata": {},
   "source": [
    "# Lesson 3: Chatbot Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d4fedc-4b90-4754-9f2d-fd3cfa321a14",
   "metadata": {},
   "source": [
    "In this lesson, you will familiarize yourself with the chatbot example you will work on during this course. The example includes the tool definitions and execution, as well as the chatbot code. Make sure to interact with the chatbot at the end of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ed96ba-5ade-4af4-9096-406ce48d5cf2",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd6bd1d4-f652-45d1-9efa-155a2cc01713",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "import json\n",
    "import os\n",
    "from typing import List\n",
    "from dotenv import load_dotenv\n",
    "import anthropic\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20f163a-87af-4e0c-87ed-1624c150c572",
   "metadata": {},
   "source": [
    "## Tool Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "549a7f46-74b3-4a1d-b084-055c99e3c318",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAPER_DIR = \"papers\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e43905e-56f3-404c-a322-f038055e9b1c",
   "metadata": {},
   "source": [
    "The first tool searches for relevant arXiv papers based on a topic and stores the papers' info in a JSON file (title, authors, summary, paper url and the publication date). The JSON files are organized by topics in the `papers` directory. The tool does not download the papers.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "886633b8-ce67-4343-822d-cc3f98f953fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_papers(topic: str, max_results: int = 5) -> List[str]:\n",
    "    \"\"\"\n",
    "    Search for papers on arXiv based on a topic and store their information.\n",
    "\n",
    "    Args:\n",
    "        topic: The topic to search for\n",
    "        max_results: Maximum number of results to retrieve (default: 5)\n",
    "\n",
    "    Returns:\n",
    "        List of paper IDs found in the search\n",
    "    \"\"\"\n",
    "\n",
    "    # Use arxiv to find the papers\n",
    "    client = arxiv.Client()\n",
    "\n",
    "    # Search for the most relevant articles matching the queried topic\n",
    "    search = arxiv.Search(\n",
    "        query=topic, max_results=max_results, sort_by=arxiv.SortCriterion.Relevance\n",
    "    )\n",
    "\n",
    "    papers = client.results(search)\n",
    "\n",
    "    # Create directory for this topic\n",
    "    path = os.path.join(PAPER_DIR, topic.lower().replace(\" \", \"_\"))\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "    file_path = os.path.join(path, \"papers_info.json\")\n",
    "\n",
    "    # Try to load existing papers info\n",
    "    try:\n",
    "        with open(file_path, \"r\") as json_file:\n",
    "            papers_info = json.load(json_file)\n",
    "    except (FileNotFoundError, json.JSONDecodeError):\n",
    "        papers_info = {}\n",
    "\n",
    "    # Process each paper and add to papers_info\n",
    "    paper_ids = []\n",
    "    for paper in papers:\n",
    "        paper_ids.append(paper.get_short_id())\n",
    "        paper_info = {\n",
    "            \"title\": paper.title,\n",
    "            \"authors\": [author.name for author in paper.authors],\n",
    "            \"summary\": paper.summary,\n",
    "            \"pdf_url\": paper.pdf_url,\n",
    "            \"published\": str(paper.published.date()),\n",
    "        }\n",
    "        papers_info[paper.get_short_id()] = paper_info\n",
    "\n",
    "    # Save updated papers_info to json file\n",
    "    with open(file_path, \"w\") as json_file:\n",
    "        json.dump(papers_info, json_file, indent=2)\n",
    "\n",
    "    print(f\"Results are saved in: {file_path}\")\n",
    "\n",
    "    return paper_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d20ee17a-afe6-438a-95b1-6e87742c7fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results are saved in: papers/computers/papers_info.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['1310.7911v2',\n",
       " 'math/9711204v1',\n",
       " '2208.00733v1',\n",
       " '2504.07020v1',\n",
       " '2403.03925v1']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_papers(\"computers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb83565-69af-47f3-9ba3-a96965cff7df",
   "metadata": {},
   "source": [
    "The second tool looks for information about a specific paper across all topic directories inside the `papers` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df9b1997-81cd-447d-9665-1cb72d93bb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_info(paper_id: str) -> str:\n",
    "    \"\"\"\n",
    "    Search for information about a specific paper across all topic directories.\n",
    "\n",
    "    Args:\n",
    "        paper_id: The ID of the paper to look for\n",
    "\n",
    "    Returns:\n",
    "        JSON string with paper information if found, error message if not found\n",
    "    \"\"\"\n",
    "\n",
    "    for item in os.listdir(PAPER_DIR):\n",
    "        item_path = os.path.join(PAPER_DIR, item)\n",
    "        if os.path.isdir(item_path):\n",
    "            file_path = os.path.join(item_path, \"papers_info.json\")\n",
    "            if os.path.isfile(file_path):\n",
    "                try:\n",
    "                    with open(file_path, \"r\") as json_file:\n",
    "                        papers_info = json.load(json_file)\n",
    "                        if paper_id in papers_info:\n",
    "                            return json.dumps(papers_info[paper_id], indent=2)\n",
    "                except (FileNotFoundError, json.JSONDecodeError) as e:\n",
    "                    print(f\"Error reading {file_path}: {str(e)}\")\n",
    "                    continue\n",
    "\n",
    "    return f\"There's no saved information related to paper {paper_id}.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ebe0de7-8f07-4e08-a670-7b371fc3d2d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n  \"title\": \"Compact manifolds with computable boundaries\",\\n  \"authors\": [\\n    \"Zvonko Iljazovic\"\\n  ],\\n  \"summary\": \"We investigate conditions under which a co-computably enumerable closed set\\\\nin a computable metric space is computable and prove that in each locally\\\\ncomputable computable metric space each co-computably enumerable compact\\\\nmanifold with computable boundary is computable. In fact, we examine the notion\\\\nof a semi-computable compact set and we prove a more general result: in any\\\\ncomputable metric space each semi-computable compact manifold with computable\\\\nboundary is computable. In particular, each semi-computable compact\\\\n(boundaryless) manifold is computable.\",\\n  \"pdf_url\": \"http://arxiv.org/pdf/1310.7911v2\",\\n  \"published\": \"2013-10-29\"\\n}'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_info(\"1310.7911v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ea3013-e690-4bc8-8622-27b4d42d61e4",
   "metadata": {},
   "source": [
    "## Tool Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7d2260-452d-472a-b56e-326479cb18c9",
   "metadata": {},
   "source": [
    "Here are the schema of each tool which you will provide to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5bdea5f-e93a-4018-8c13-00d5ee10c0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    {\n",
    "        \"name\": \"search_papers\",\n",
    "        \"description\": \"Search for papers on arXiv based on a topic and store their information.\",\n",
    "        \"input_schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"topic\": {\"type\": \"string\", \"description\": \"The topic to search for\"},\n",
    "                \"max_results\": {\n",
    "                    \"type\": \"integer\",\n",
    "                    \"description\": \"Maximum number of results to retrieve\",\n",
    "                    \"default\": 5,\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"topic\"],\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"extract_info\",\n",
    "        \"description\": \"Search for information about a specific paper across all topic directories.\",\n",
    "        \"input_schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"paper_id\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The ID of the paper to look for\",\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"paper_id\"],\n",
    "        },\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec668d24-1559-41b7-bc8a-e2dca77dfaf2",
   "metadata": {},
   "source": [
    "## Tool Mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c728c1ec-36b1-48b4-9f85-622464ac79f4",
   "metadata": {},
   "source": [
    "This code handles tool mapping and execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c90790c0-efc4-4068-9c00-d2592d80bc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_tool_function = {\"search_papers\": search_papers, \"extract_info\": extract_info}\n",
    "\n",
    "\n",
    "def execute_tool(tool_name, tool_args):\n",
    "\n",
    "    result = mapping_tool_function[tool_name](**tool_args)\n",
    "\n",
    "    if result is None:\n",
    "        result = \"The operation completed but didn't return any results.\"\n",
    "\n",
    "    elif isinstance(result, list):\n",
    "        result = \", \".join(result)\n",
    "\n",
    "    elif isinstance(result, dict):\n",
    "        # Convert dictionaries to formatted JSON strings\n",
    "        result = json.dumps(result, indent=2)\n",
    "\n",
    "    else:\n",
    "        # For any other type, convert using str()\n",
    "        result = str(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8fc4d3-58ac-482c-8bbd-bccd6ef9fc31",
   "metadata": {},
   "source": [
    "## Chatbot Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ba0fad-b0e4-4415-a431-341e9ca85087",
   "metadata": {},
   "source": [
    "The chatbot handles the user's queries one by one, but it does not persist memory across the queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe662400-8506-464e-a3da-75a3d8848bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "client = anthropic.Anthropic()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175586b4-acdf-4103-8039-134478a4f797",
   "metadata": {},
   "source": [
    "### Query Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12a896e0-3f56-417e-aa51-c61756048593",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_query(query):\n",
    "\n",
    "    messages = [{\"role\": \"user\", \"content\": query}]\n",
    "\n",
    "    response = client.messages.create(\n",
    "        max_tokens=2024,\n",
    "        model=\"claude-3-7-sonnet-20250219\",\n",
    "        tools=tools,\n",
    "        messages=messages,\n",
    "    )\n",
    "\n",
    "    process_query = True\n",
    "    while process_query:\n",
    "        assistant_content = []\n",
    "\n",
    "        for content in response.content:\n",
    "            if content.type == \"text\":\n",
    "\n",
    "                display(Markdown(content.text))\n",
    "                assistant_content.append(content)\n",
    "\n",
    "                if len(response.content) == 1:\n",
    "                    process_query = False\n",
    "\n",
    "            elif content.type == \"tool_use\":\n",
    "\n",
    "                assistant_content.append(content)\n",
    "                messages.append({\"role\": \"assistant\", \"content\": assistant_content})\n",
    "\n",
    "                tool_id = content.id\n",
    "                tool_args = content.input\n",
    "                tool_name = content.name\n",
    "                print(f\"Calling tool {tool_name} with args {tool_args}\")\n",
    "\n",
    "                result = execute_tool(tool_name, tool_args)\n",
    "                messages.append(\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": [\n",
    "                            {\n",
    "                                \"type\": \"tool_result\",\n",
    "                                \"tool_use_id\": tool_id,\n",
    "                                \"content\": result,\n",
    "                            }\n",
    "                        ],\n",
    "                    }\n",
    "                )\n",
    "                response = client.messages.create(\n",
    "                    max_tokens=2024,\n",
    "                    model=\"claude-3-7-sonnet-20250219\",\n",
    "                    tools=tools,\n",
    "                    messages=messages,\n",
    "                )\n",
    "\n",
    "                if len(response.content) == 1 and response.content[0].type == \"text\":\n",
    "                    display(Markdown(response.content[0].text))\n",
    "                    process_query = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2921ee7f-d2be-464b-ab7b-8db2a3c13ba9",
   "metadata": {},
   "source": [
    "### Chat Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16979cdc-81e9-432b-ba7f-e810b52961e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_loop():\n",
    "    print(\"Type your queries or 'quit' to exit.\")\n",
    "    while True:\n",
    "        try:\n",
    "            query = input(\"\\nQuery: \").strip()\n",
    "            if query.lower() == \"quit\":\n",
    "                break\n",
    "\n",
    "            process_query(query)\n",
    "            print(\"\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfaf254-f22a-4951-885e-1d21fbc41ff3",
   "metadata": {},
   "source": [
    "Feel free to interact with the chatbot. Here's an example query: \n",
    "\n",
    "- Search for 2 papers on \"LLM interpretability\"\n",
    "\n",
    "To access the `papers` folder: 1) click on the `File` option on the top menu of the notebook and 2) click on `Open` and then 3) click on `L3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39676f70-1c72-4da3-8363-da281bd5a83e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type your queries or 'quit' to exit.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Query:  hello there\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Hello! I'm here to help you with searching for and extracting information about research papers on arXiv. \n",
       "\n",
       "Is there a specific topic you're interested in learning about? Or perhaps you have a particular paper ID you'd like more information on? Let me know what you're looking for, and I can assist you with finding relevant research papers or details about a specific paper."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Query:  I'd like to know about the most impactful papers regarding LLMs\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "I'd be happy to help you find information about the most impactful papers regarding Large Language Models (LLMs). Let me search for relevant papers on this topic."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling tool search_papers with args {'topic': 'large language models', 'max_results': 5}\n",
      "Results are saved in: papers/large_language_models/papers_info.json\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Now, let me get more detailed information about each of these papers to help you understand their impact and contributions to the field of LLMs."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling tool extract_info with args {'paper_id': '2306.07377v1'}\n",
      "Calling tool extract_info with args {'paper_id': '2202.03371v1'}\n",
      "Calling tool extract_info with args {'paper_id': '2305.06530v1'}\n",
      "Calling tool extract_info with args {'paper_id': '2408.10441v1'}\n",
      "Calling tool extract_info with args {'paper_id': '2404.09579v1'}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Let me search for some of the truly seminal papers in the LLM field, as the initial search might not have captured the most impactful ones:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling tool search_papers with args {'topic': 'transformer attention is all you need', 'max_results': 3}\n",
      "Results are saved in: papers/transformer_attention_is_all_you_need/papers_info.json\n",
      "Calling tool extract_info with args {'paper_id': '2405.09285v1'}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Let me try another search for more foundational LLM papers:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling tool search_papers with args {'topic': 'GPT-3 language models few-shot learning', 'max_results': 2}\n",
      "Results are saved in: papers/gpt-3_language_models_few-shot_learning/papers_info.json\n",
      "Calling tool extract_info with args {'paper_id': '2205.10981v1'}\n",
      "Calling tool extract_info with args {'paper_id': '2212.00857v1'}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Let me try one more search to find the original Transformer paper:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling tool search_papers with args {'topic': 'Attention is All You Need', 'max_results': 1}\n",
      "Results are saved in: papers/attention_is_all_you_need/papers_info.json\n",
      "Calling tool extract_info with args {'paper_id': '2407.15516v1'}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Based on my searches, let me provide you with information about the most impactful papers regarding Large Language Models (LLMs):\n",
       "\n",
       "# Most Impactful Papers on Large Language Models (LLMs)\n",
       "\n",
       "While my search didn't surface all the original foundational papers, I can share what I found and supplement with knowledge about the most influential LLM papers:\n",
       "\n",
       "## Foundational Papers\n",
       "\n",
       "1. **\"Attention Is All You Need\" (2017)** - While not directly retrieved in my search, this paper by Vaswani et al. introduced the Transformer architecture, which is the foundation for all modern LLMs. It revolutionized NLP by replacing recurrent neural networks with attention mechanisms.\n",
       "\n",
       "2. **\"Language Models are Few-Shot Learners\" (2020)** - The GPT-3 paper by Brown et al. This groundbreaking work demonstrated that scaling language models to 175 billion parameters enables few-shot learning, where models can perform tasks with minimal examples.\n",
       "\n",
       "3. **\"Training language models to follow instructions with human feedback\" (2022)** - The RLHF (Reinforcement Learning from Human Feedback) paper by OpenAI that introduced the methodology behind ChatGPT, making LLMs more aligned with human intent.\n",
       "\n",
       "## Papers Found in My Search\n",
       "\n",
       "1. **\"A survey on GPT-3\" (2022)** - By Zong and Krishnamachari, this paper provides an overview of GPT-3's capabilities, applications across domains, and challenges it faces. It documents the impact of one of the most influential LLMs.\n",
       "\n",
       "2. **\"Lost in Translation: Large Language Models in Non-English Content Analysis\" (2023)** - This paper highlights the challenges of LLMs in languages other than English and discusses multilingual language models.\n",
       "\n",
       "3. **\"Attention Is All You Need But You Don't Need All Of It For Inference of Large Language Models\" (2024)** - A recent paper showing how LLM inference can be optimized by dropping certain attention layers with minimal performance impact.\n",
       "\n",
       "4. **\"Goldfish: Monolingual Language Models for 350 Languages\" (2024)** - Addresses the challenge of creating language models for low-resource languages, an important direction for making LLM technology more inclusive.\n",
       "\n",
       "## Key Impact Areas\n",
       "\n",
       "The most impactful LLM papers have contributed to:\n",
       "\n",
       "1. **Architecture Innovation**: The Transformer architecture revolutionized how we process sequential data.\n",
       "\n",
       "2. **Scaling Laws**: Papers demonstrating how model performance improves with increased parameters, data, and compute.\n",
       "\n",
       "3. **Emergent Abilities**: Research showing how capabilities like reasoning, in-context learning, and instruction following emerge at scale.\n",
       "\n",
       "4. **Alignment and Safety**: Methods to align LLMs with human values and reduce harmful outputs.\n",
       "\n",
       "5. **Multimodality**: Extending LLMs to work with images, audio, and other modalities.\n",
       "\n",
       "Would you like me to search for more specific foundational papers or focus on a particular aspect of LLM research?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chat_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c3f9671a-77e5-481e-898a-998018d9146f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "papers/attention_is_all_you_need:\n",
      "papers_info.json\n",
      "\n",
      "papers/computers:\n",
      "papers_info.json\n",
      "\n",
      "papers/gpt-3_language_models_few-shot_learning:\n",
      "papers_info.json\n",
      "\n",
      "papers/large_language_models:\n",
      "papers_info.json\n",
      "\n",
      "papers/transformer_attention_is_all_you_need:\n",
      "papers_info.json\n"
     ]
    }
   ],
   "source": [
    "!ls papers/*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34df7890-4b4c-4ec9-b06f-abc8c4a290e8",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#f7fff8; padding:15px; border-width:3px; border-color:#e0f0e0; border-style:solid; border-radius:6px\"> ðŸš¨\n",
    "&nbsp; <b>Different Run Results:</b> The output generated by AI chat models can vary with each execution due to their dynamic, probabilistic nature. Don't be surprised if your results differ from those shown in the video.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb34ee2d",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6ff; padding:13px; border-width:3px; border-color:#efe6ef; border-style:solid; border-radius:6px\">\n",
    "<p> ðŸ’» &nbsp; <b> To Access the <code>requirements.txt</code> file or the <code>papers</code> folder: </b> 1) click on the <em>\"File\"</em> option on the top menu of the notebook and then 2) click on <em>\"Open\"</em> and finally 3) click on <em>\"L3\"</em>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508916f3-8fa1-4e21-bfa7-081a810bc36c",
   "metadata": {},
   "source": [
    "In the next lessons, you will take out the tool definitions to wrap them in an MCP server. Then you will create an MCP client inside the chatbot to make the chatbot MCP compatible.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02d207b-e07d-49ff-bb03-7954aa86c167",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "[Guide on how to implement tool use](https://docs.anthropic.com/en/docs/build-with-claude/tool-use/overview#how-to-implement-tool-use)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e5135e-01c3-4632-9f83-a1e6dd811049",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6ff; padding:13px; border-width:3px; border-color:#efe6ef; border-style:solid; border-radius:6px\">\n",
    "\n",
    "\n",
    "<p> â¬‡ &nbsp; <b>Download Notebooks:</b> 1) click on the <em>\"File\"</em> option on the top menu of the notebook and then 2) click on <em>\"Download as\"</em> and select <em>\"Notebook (.ipynb)\"</em>.</p>\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
